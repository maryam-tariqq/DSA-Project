# -*- coding: utf-8 -*-
"""dynamic_indexing.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1TPFsKBBTT7BpU2NC8QQtsPlZiQf5oCXp
"""

from google.colab import drive
drive.mount('/content/drive')

# dynamic_indexer.py - UPDATED FOR YOUR PROJECT
# ✓ <1 minute indexing
# ✓ Proper field frequency updates
# ✓ Memory efficient (loads only needed barrels)
# ✓ Compatible with your search engine

import json
import os
from datetime import datetime
from nltk.stem import PorterStemmer
from collections import defaultdict
import time

class DynamicIndexer:
    """
    OPTIMIZED: Only loads/saves barrels that are actually modified
    Properly updates all field frequencies for ranking
    Compatible with search_engine_integrated.py
    """

    def __init__(self, base_path=None):
        # Auto-detect path or use provided
        if base_path is None:
            # Try to find the project structure
            if os.path.exists("/content/drive/MyDrive/DSA-Project"):
                base_path = "/content/drive/MyDrive/DSA-Project/data/processed/"
            else:
                # Fallback for local development
                base_path = "./data/processed/"

        self.base_path = base_path
        self.lexicon_file = os.path.join(base_path, "lexicon.json")
        self.forward_index_file = os.path.join(base_path, "forward_index.json")
        self.barrels_folder = os.path.join(base_path, "barrels")

        # Document metadata file
        raw_path = base_path.replace("/processed/", "/raw/")
        self.doc_meta_file = os.path.join(raw_path, "arxiv_100k.json")

        self.stemmer = PorterStemmer()

        # Field mapping for barrel entries
        # [total_freq, positions, title_freq, authors_freq, categories_freq, abstract_freq]
        self.field_map = {
            "title": 2,
            "authors": 3,
            "categories": 4,
            "abstract": 5
        }

        self._load_data()

    def _load_data(self):
        """Load only essential files"""
        print("Loading index files...")

        try:
            with open(self.lexicon_file, 'r', encoding='utf-8') as f:
                self.lexicon = json.load(f)
        except FileNotFoundError:
            print(f"⚠ Lexicon not found at {self.lexicon_file}, creating new")
            self.lexicon = {}

        try:
            with open(self.forward_index_file, 'r', encoding='utf-8') as f:
                self.forward_index = json.load(f)
        except FileNotFoundError:
            print(f"⚠ Forward index not found, creating new")
            self.forward_index = {}

        # Load doc metadata
        if os.path.exists(self.doc_meta_file):
            with open(self.doc_meta_file, 'r') as f:
                docs = json.load(f)
                self.doc_meta = {d["id"]: d for d in docs}
        else:
            print(f"⚠ Doc metadata not found, creating new")
            self.doc_meta = {}

        # Ensure barrels folder exists
        os.makedirs(self.barrels_folder, exist_ok=True)

        print(f"✓ Ready: {len(self.lexicon):,} terms, {len(self.forward_index):,} docs")

    # ==================== ADD DOCUMENTS ====================

    def add(self, data, format_type="auto"):
        """Add document(s) - completes in <1 minute"""
        start_time = time.time()

        if format_type == "auto":
            format_type = self._detect_format(data)

        count = 0
        if format_type == "dict":
            if isinstance(data, list):
                count = sum(1 for d in data if self._add_document(self._normalize_fields(d)))
            else:
                count = 1 if self._add_document(self._normalize_fields(data)) else 0

        elif format_type == "json":
            doc = json.loads(data)
            if isinstance(doc, list):
                count = sum(1 for d in doc if self._add_document(self._normalize_fields(d)))
            else:
                count = 1 if self._add_document(self._normalize_fields(doc)) else 0

        elif format_type == "json_file":
            with open(data, 'r', encoding='utf-8') as f:
                doc = json.load(f)
            if isinstance(doc, list):
                count = sum(1 for d in doc if self._add_document(self._normalize_fields(d)))
            else:
                count = 1 if self._add_document(self._normalize_fields(doc)) else 0

        elapsed = time.time() - start_time
        print(f"✓ Indexed {count} document(s) in {elapsed:.2f}s")
        return count > 0

    def _detect_format(self, data):
        if isinstance(data, dict) or isinstance(data, list):
            return "dict"
        if isinstance(data, str):
            if os.path.exists(data):
                return "json_file" if data.endswith(".json") else "text"
            try:
                json.loads(data)
                return "json"
            except:
                return "text"
        return "unknown"

    def _normalize_fields(self, doc):
        """Normalize document fields to match your schema"""
        return {
            'id': str(doc.get('id', f"AUTO_{int(time.time() * 1000)}")),
            'title': str(doc.get('title', '')),
            'authors': str(doc.get('authors', '')),
            'categories': str(doc.get('categories', '')),
            'abstract': str(doc.get('abstract', '')),
            'update_date': doc.get('update_date', datetime.now().strftime("%Y-%m-%d")),
            'paper_url': doc.get('paper_url', f"https://arxiv.org/abs/{doc.get('id', '')}")
        }

    # ==================== CORE INDEXING (FIXED) ====================

    def _add_document(self, doc):
        """Add single document - OPTIMIZED"""
        doc_id = str(doc["id"])

        # Check if exists
        if doc_id in self.forward_index:
            print(f"⚠ Document {doc_id} already exists")
            return False

        # Tokenize
        tokens = self._preprocess_document(doc)

        if not tokens:
            print(f"⚠ No valid tokens for document {doc_id}")
            return False

        # Update lexicon (add new words)
        next_id = max(self.lexicon.values(), default=0) + 1
        for t in tokens:
            if t["token"] not in self.lexicon:
                self.lexicon[t["token"]] = next_id
                next_id += 1

        # Update forward index
        self._update_forward_index(doc_id, tokens)

        # Update barrels (ONLY modified ones)
        self._update_barrels_optimized(doc_id, tokens)

        # Update metadata
        self.doc_meta[doc_id] = doc

        # Save all
        self._save_all()

        return True

    def _preprocess_document(self, doc):
        """Tokenize with field information"""
        tokens = []
        pos = 0

        fields = [
            ("title", doc.get("title", "")),
            ("authors", doc.get("authors", "")),
            ("categories", doc.get("categories", "")),
            ("abstract", doc.get("abstract", ""))
        ]

        for field, text in fields:
            words = str(text).lower().split()
            for word in words:
                # Clean word
                word = ''.join(c for c in word if c.isalnum() or c == '-')
                if len(word) > 1:
                    try:
                        stemmed = self.stemmer.stem(word)
                        tokens.append({
                            "token": stemmed,
                            "global_pos": pos,
                            "field": field
                        })
                        pos += 1
                    except Exception as e:
                        # Skip problematic words
                        continue

        return tokens

    def _update_forward_index(self, doc_id, tokens):
        """Update forward index with proper field counts"""
        data = defaultdict(lambda: [0, [], 0, 0, 0, 0])

        for t in tokens:
            wid = str(self.lexicon[t["token"]])
            entry = data[wid]

            # Update total frequency
            entry[0] += 1

            # Update positions
            entry[1].append(t["global_pos"])

            # Update field-specific frequencies
            field_idx = self.field_map.get(t["field"])
            if field_idx is not None:
                entry[field_idx] += 1

        self.forward_index[doc_id] = dict(data)

    def _update_barrels_optimized(self, doc_id, tokens):
        """
        OPTIMIZED: Only load/save barrels that are actually modified
        Properly updates ALL fields: [total_freq, positions, title, authors, categories, abstract]
        """
        # Group tokens by word to get per-word stats
        word_stats = defaultdict(lambda: {
            "total": 0,
            "positions": [],
            "title": 0,
            "authors": 0,
            "categories": 0,
            "abstract": 0
        })

        for t in tokens:
            word = t["token"]
            word_stats[word]["total"] += 1
            word_stats[word]["positions"].append(t["global_pos"])

            # Update field counts
            field = t["field"]
            if field in word_stats[word]:
                word_stats[word][field] += 1

        # Group by barrel letter (to minimize file I/O)
        barrels_to_update = defaultdict(dict)

        for word, stats in word_stats.items():
            wid = str(self.lexicon[word])
            letter = word[0] if word and word[0].isalpha() else "#"

            # Format barrel entry correctly
            entry = [
                stats["total"],        # [0] total frequency
                stats["positions"],    # [1] positions list
                stats["title"],        # [2] title frequency
                stats["authors"],      # [3] authors frequency
                stats["categories"],   # [4] categories frequency
                stats["abstract"]      # [5] abstract frequency
            ]

            barrels_to_update[letter][wid] = entry

        # Update only modified barrels
        for letter, word_entries in barrels_to_update.items():
            barrel_path = os.path.join(self.barrels_folder, f"{letter}.json")

            # Load barrel
            if os.path.exists(barrel_path):
                with open(barrel_path, 'r') as f:
                    barrel = json.load(f)
            else:
                barrel = {}

            # Update barrel with new document entries
            for wid, entry in word_entries.items():
                if wid not in barrel:
                    barrel[wid] = {}
                barrel[wid][doc_id] = entry

            # Save barrel
            with open(barrel_path, 'w') as f:
                json.dump(barrel, f, separators=(',', ':'))

    def _save_all(self):
        """Save all index files"""
        # Save lexicon
        with open(self.lexicon_file, 'w') as f:
            json.dump(self.lexicon, f, separators=(',', ':'))

        # Save forward index
        with open(self.forward_index_file, 'w') as f:
            json.dump(self.forward_index, f, separators=(',', ':'))

        # Save metadata
        with open(self.doc_meta_file, 'w') as f:
            json.dump(list(self.doc_meta.values()), f, indent=2)


# ==================== USAGE ====================

if __name__ == "__main__":
    indexer = DynamicIndexer()

    # Test adding a document
    new_doc = {
        "id": "2025.12345",
        "title": "Advanced Machine Learning Techniques for Neural Networks",
        "authors": "John Doe, Jane Smith",
        "categories": "cs.LG cs.AI",
        "abstract": "This paper presents novel machine learning techniques for improving neural network performance..."
    }

    print("\nAdding test document...")
    indexer.add(new_doc)

    print("\n✓ Indexing complete!")
    print(f"  Total documents: {len(indexer.forward_index):,}")
    print(f"  Total terms: {len(indexer.lexicon):,}")