{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"PUs-LY9Kg1H3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import json\n","import os\n","from datetime import datetime\n","from nltk.stem import PorterStemmer\n","from collections import defaultdict\n","import time\n","\n","\n","class CompleteSearchEngine:\n","    \"\"\"\n","    Search engine supporting dynamic content addition\n","    with automatic indexing and ranked retrieval.\n","    \"\"\"\n","\n","    def __init__(self, base_path=\"/content/drive/MyDrive/DSA-Project/data/processed/\"):\n","        self.base_path = base_path\n","        self.preprocessing_file = os.path.join(base_path, \"preprocessing.json\")\n","        self.lexicon_file = os.path.join(base_path, \"lexicon.json\")\n","        self.forward_index_file = os.path.join(base_path, \"forward_index.json\")\n","        self.barrels_folder = os.path.join(base_path, \"barrels\")\n","\n","        self.stemmer = PorterStemmer()\n","        self.field_map = {\n","            \"title\": 2, \"authors\": 3, \"categories\": 4,\n","            \"report_no\": 5, \"journal\": 6, \"abstract\": 7, \"update_date\": 8\n","        }\n","\n","        self.barrel_cache = {}\n","        self._load_data()\n","\n","    def _load_data(self):\n","        with open(self.preprocessing_file, 'r', encoding='utf-8') as f:\n","            self.preprocessed_docs = json.load(f)\n","\n","        with open(self.lexicon_file, 'r', encoding='utf-8') as f:\n","            self.lexicon = json.load(f)\n","\n","        with open(self.forward_index_file, 'r', encoding='utf-8') as f:\n","            self.forward_index = json.load(f)\n","\n","    # ================= INPUT HANDLING =================\n","\n","    def add(self, data, format_type=\"auto\"):\n","        if format_type == \"auto\":\n","            format_type = self._detect_format(data)\n","\n","        if format_type == \"dict\":\n","            if isinstance(data, list):\n","                return self._add_batch(data)\n","            return self._add_document(self._normalize_fields(data))\n","\n","        elif format_type == \"json\":\n","            doc = json.loads(data)\n","            if isinstance(doc, list):\n","                return self._add_batch(doc)\n","            return self._add_document(self._normalize_fields(doc))\n","\n","        elif format_type == \"json_file\":\n","            with open(data, 'r', encoding='utf-8') as f:\n","                doc = json.load(f)\n","            if isinstance(doc, list):\n","                return self._add_batch(doc)\n","            return self._add_document(self._normalize_fields(doc))\n","\n","        elif format_type == \"text\":\n","            if os.path.exists(data):\n","                with open(data, 'r', encoding='utf-8') as f:\n","                    data = f.read()\n","            return self._add_from_text(data)\n","\n","        elif format_type == \"csv\":\n","            import csv\n","            with open(data, 'r', encoding='utf-8') as f:\n","                docs = list(csv.DictReader(f))\n","            return self._add_batch(docs)\n","\n","        return False\n","\n","    def _detect_format(self, data):\n","        if isinstance(data, dict) or isinstance(data, list):\n","            return \"dict\"\n","        if isinstance(data, str):\n","            if os.path.exists(data):\n","                if data.endswith(\".json\"):\n","                    return \"json_file\"\n","                if data.endswith(\".csv\"):\n","                    return \"csv\"\n","                return \"text\"\n","            try:\n","                json.loads(data)\n","                return \"json\"\n","            except:\n","                return \"text\"\n","        return \"unknown\"\n","\n","    def _normalize_fields(self, doc):\n","        mappings = {\n","            'id': ['id', 'doc_id'],\n","            'title': ['title'],\n","            'authors': ['authors', 'author'],\n","            'categories': ['categories', 'category'],\n","            'abstract': ['abstract', 'description', 'content'],\n","            'report-no': ['report-no'],\n","            'journal-ref': ['journal-ref', 'journal'],\n","            'update_date': ['update_date', 'date']\n","        }\n","\n","        normalized = {}\n","        for std, vars in mappings.items():\n","            for v in vars:\n","                if v in doc:\n","                    normalized[std] = doc[v]\n","                    break\n","            if std not in normalized:\n","                normalized[std] = datetime.now().strftime(\"%Y-%m-%d\") if std == \"update_date\" else \"\"\n","\n","        if not normalized.get(\"id\"):\n","            normalized[\"id\"] = f\"AUTO_{datetime.now().strftime('%Y%m%d%H%M%S')}\"\n","\n","        return normalized\n","\n","    def _add_from_text(self, text):\n","        return self._add_document({\n","            \"id\": f\"TEXT_{datetime.now().strftime('%Y%m%d%H%M%S')}\",\n","            \"title\": text.split('\\n')[0][:200],\n","            \"authors\": \"Unknown\",\n","            \"categories\": \"misc\",\n","            \"abstract\": text[:500],\n","            \"report-no\": \"\",\n","            \"journal-ref\": \"\",\n","            \"update_date\": datetime.now().strftime(\"%Y-%m-%d\")\n","        })\n","\n","    # ================= CORE INDEXING =================\n","\n","    def _add_document(self, doc):\n","        doc_id = str(doc[\"id\"])\n","        if doc_id in self.forward_index:\n","            return False\n","\n","        tokens = self._preprocess_document(doc)\n","        self.preprocessed_docs.append({\n","            \"id\": doc_id,\n","            \"title\": doc.get(\"title\", \"\"),\n","            \"authors\": doc.get(\"authors\", \"\"),\n","            \"categories\": doc.get(\"categories\", \"\"),\n","            \"abstract\": doc.get(\"abstract\", \"\"),\n","            \"tokens\": tokens\n","        })\n","\n","        next_id = max(self.lexicon.values(), default=0) + 1\n","        for t in tokens:\n","            if t[\"token\"] not in self.lexicon:\n","                self.lexicon[t[\"token\"]] = next_id\n","                next_id += 1\n","\n","        self._update_forward_index(doc_id, tokens)\n","        self._update_barrels(doc_id, tokens)\n","        self._save_all()\n","        self.barrel_cache.clear()\n","        return True\n","\n","    def _add_batch(self, docs):\n","        return any(self._add_document(self._normalize_fields(d)) for d in docs)\n","\n","    def _preprocess_document(self, doc):\n","        tokens, pos = [], 0\n","        fields = [\n","            (\"title\", doc.get(\"title\", \"\")),\n","            (\"authors\", doc.get(\"authors\", \"\")),\n","            (\"categories\", doc.get(\"categories\", \"\")),\n","            (\"report_no\", doc.get(\"report-no\", \"\")),\n","            (\"journal\", doc.get(\"journal-ref\", \"\")),\n","            (\"abstract\", doc.get(\"abstract\", \"\")),\n","        ]\n","\n","        for field, text in fields:\n","            for word in text.lower().split():\n","                word = word.strip(\".,!?()[]{}'\\\"\")\n","                if len(word) > 1:\n","                    tokens.append({\n","                        \"token\": self.stemmer.stem(word),\n","                        \"global_pos\": pos,\n","                        \"field\": field\n","                    })\n","                    pos += 1\n","        return tokens\n","\n","    def _update_forward_index(self, doc_id, tokens):\n","        data = defaultdict(lambda: [0, [], 0, 0, 0, 0, 0, 0, 0])\n","        for t in tokens:\n","            wid = str(self.lexicon[t[\"token\"]])\n","            entry = data[wid]\n","            entry[0] += 1\n","            entry[1].append(t[\"global_pos\"])\n","            if t[\"field\"] in self.field_map:\n","                entry[self.field_map[t[\"field\"]]] += 1\n","        self.forward_index[doc_id] = dict(data)\n","\n","    def _update_barrels(self, doc_id, tokens):\n","        letters = \"abcdefghijklmnopqrstuvwxyz#\"\n","        barrels = {l: {} for l in letters}\n","\n","        for l in letters:\n","            path = os.path.join(self.barrels_folder, f\"{l}.json\")\n","            if os.path.exists(path):\n","                with open(path, 'r') as f:\n","                    barrels[l] = json.load(f)\n","\n","        postings = defaultdict(lambda: [0, [], 0, 0, 0, 0, 0, 0, 0])\n","        for t in tokens:\n","            wid = str(self.lexicon[t[\"token\"]])\n","            postings[wid][0] += 1\n","\n","        for wid, post in postings.items():\n","            word = next(k for k, v in self.lexicon.items() if str(v) == wid)\n","            key = word[0] if word[0].isalpha() else \"#\"\n","            barrels[key].setdefault(wid, {})[doc_id] = post\n","\n","        for k, v in barrels.items():\n","            with open(os.path.join(self.barrels_folder, f\"{k}.json\"), 'w') as f:\n","                json.dump(v, f, separators=(',', ':'))\n","\n","    def _save_all(self):\n","        json.dump(self.preprocessed_docs, open(self.preprocessing_file, 'w'), indent=2)\n","        json.dump(self.lexicon, open(self.lexicon_file, 'w'), indent=2)\n","        json.dump(self.forward_index, open(self.forward_index_file, 'w'), separators=(',', ':'))\n","\n","    # ================= SEARCH =================\n","\n","    def search(self, query, top_k=10):\n","        terms = [self.stemmer.stem(w.lower()) for w in query.split()]\n","        postings = [self._get_postings(t) for t in terms if self._get_postings(t)]\n","\n","        if not postings:\n","            return []\n","\n","        docs = set(postings[0])\n","        for p in postings[1:]:\n","            docs &= set(p)\n","\n","        ranked = []\n","        for d in docs:\n","            score = sum(p[d][0] for p in postings if d in p)\n","            length = sum(v[0] for v in self.forward_index[d].values())\n","            ranked.append((d, score / length if length else 0))\n","\n","        ranked.sort(key=lambda x: x[1], reverse=True)\n","        return ranked[:top_k]\n","\n","    def _get_postings(self, word):\n","        wid = self.lexicon.get(word)\n","        if not wid:\n","            return {}\n","        letter = word[0] if word[0].isalpha() else \"#\"\n","        if letter not in self.barrel_cache:\n","            with open(os.path.join(self.barrels_folder, f\"{letter}.json\")) as f:\n","                self.barrel_cache[letter] = json.load(f)\n","        return self.barrel_cache[letter].get(str(wid), {})\n"],"metadata":{"id":"dfVVPnPrkYKV"},"execution_count":null,"outputs":[]}]}