# -*- coding: utf-8 -*-
"""inverted_index.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1xCmPbD_Y6LcsFcx23d5Tgt7zf35sk653
"""



import json
import nltk
from nltk.corpus import stopwords
from collections import defaultdict
import os

# Download stopwords quietly so the script can run without errors
nltk.download('stopwords', quiet=True)

INPUT_FILE = "../../data/processed/preprocessing.json"
OUTPUT_FILE = "../../data/processed/inverted_index.json"

stop_words = set(stopwords.words('english'))


def load_preprocessed_data(file_path):
    # This function simply reads the preprocessed JSON file.
    # If the file is missing or broken, it returns an empty list instead of crashing.
    try:
        with open(file_path, "r", encoding="utf-8") as f:
            return json.load(f)
    except:
        print("Error: Could not load preprocessed data.")
        return []


def load_existing_index(file_path):
    # This function loads the already saved inverted index if it exists.
    # It also collects document IDs that are already included, so we donâ€™t reprocess them.
    if not os.path.exists(file_path):
        return {}, set()

    try:
        with open(file_path, "r", encoding="utf-8") as f:
            inverted_index = json.load(f)

        processed_ids = set()
        for doc_list in inverted_index.values():
            processed_ids.update(doc_list)

        return inverted_index, processed_ids
    except:
        return {}, set()


def build_inverted_index(docs, existing_index, processed_ids):
    # This function updates the inverted index.
    # It looks at each document and adds only new document IDs for each token.
    # Documents already processed earlier are skipped.
    inverted_index = defaultdict(list, existing_index)
    new_docs_count = 0

    for doc in docs:
        doc_id = str(doc.get("id"))

        # Skip documents that were already indexed before
        if doc_id in processed_ids:
            continue

        new_docs_count += 1
        tokens = doc.get("tokens", [])

        # Using a set here avoids adding the same token twice from the same document
        for token in set(tokens):
            if doc_id not in inverted_index[token]:
                inverted_index[token].append(doc_id)

    return dict(inverted_index), new_docs_count


def save_inverted_index(inverted_index, output_file):
    # This function saves the final inverted index into a JSON file.
    # If the folder does not exist, it creates it.
    try:
        os.makedirs(os.path.dirname(output_file), exist_ok=True)
        with open(output_file, "w", encoding="utf-8") as f:
            json.dump(inverted_index, f, indent=2, ensure_ascii=False)
        print("Inverted index saved successfully.")
    except Exception as e:
        print("Error while saving inverted index:", e)


def main():
    # The program starts here. First we load the existing index if it exists.
    existing_index, processed_ids = load_existing_index(OUTPUT_FILE)

    # Next we load the preprocessed documents that contain the tokens
    docs = load_preprocessed_data(INPUT_FILE)

    if not docs:
        print("No documents found. Run preprocessing first.")
        return

    # We now build or update the inverted index using only new documents
    inverted_index, new_docs_count = build_inverted_index(
        docs, existing_index, processed_ids
    )

    if new_docs_count == 0:
        print("No new documents to add to the index.")
        return

    print(f"Indexed {new_docs_count} new documents.")

    # Finally we save the updated index
    save_inverted_index(inverted_index, OUTPUT_FILE)


if __name__ == "__main__":
    main()